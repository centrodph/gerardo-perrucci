"use strict";(self.webpackChunkweb=self.webpackChunkweb||[]).push([[1219],{3230:n=>{n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"machine-learning/visualization-missing-values","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/visualization-missing-values","source":"@site/blog/machine-learning/2024-06-02-machine-leaning-visualization-missing-values.md","title":"Machine Learning Visualization Missing Values","description":"Visualizing Missing Data: A Step-by-Step Guide","date":"2024-06-02T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"Data Visualization","permalink":"/gerardo-perrucci/blog/tags/data-visualization"},{"label":"Data Preprocessing","permalink":"/gerardo-perrucci/blog/tags/data-preprocessing"},{"label":"missingno","permalink":"/gerardo-perrucci/blog/tags/missingno"}],"readingTime":3.59,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/visualization-missing-values","title":"Machine Learning Visualization Missing Values","authors":["me"],"tags":["Machine Learning","Data Visualization","Data Preprocessing","missingno"],"image":"./ml-visualization-missing-values-heatmap.png"},"unlisted":false,"nextItem":{"title":"Machine Learning References and Code Examples","permalink":"/gerardo-perrucci/blog/machine-learning/references-and-code-examples"}},"content":"## Visualizing Missing Data: A Step-by-Step Guide\\n\\nHandling missing data is crucial in data analysis and machine learning. Visualizing missing data helps to understand the extent and pattern of missingness, which can inform the choice of strategies for dealing with it. In this guide, we\'ll use Python and the `missingno` library to visualize missing data in a dataset.\\n\\n![Visualization missing values with missingno](./ml-visualization-missing-values.png)\\n\\nYou can download the Jupiter notebook of the example [Visualizing Missing Data](https://github.com/centrodph/ml/blob/main/data-processing/Visualization%20missing%20data%20techniques.ipynb)\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Importing Libraries\\n\\nFirst, we need to import the necessary libraries for data manipulation and visualization.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nimport missingno as msno\\n```\\n\\n- **Pandas**: A powerful data manipulation library.\\n- **NumPy**: A fundamental package for numerical computations.\\n- **Missingno**: A library for visualizing missing data.\\n\\n### 2. Loading the Data\\n\\nWe load the dataset into a Pandas DataFrame. For this example, we\'ll use a dataset that comes with the `missingno` library.\\nSame data that we used in the previous article [Machine Learning Handling Missing Values](./2024-06-01-machine-learning-handling-missing-values.md). You can download the dataset from the [Kaggle website](https://www.kaggle.com/code/alexisbcook/handling-missing-values/data?select=NFL+Play+by+Play+2009-2017+%28v4%29.csv)\\n\\n```python\\nsf_permits = pd.read_csv(\\"./Building_Permits.csv\\")\\n\\nsf_permits.head()\\n```\\n\\n    /tmp/ipykernel_50336/2707110962.py:1: DtypeWarning: Columns (22,32) have mixed types. Specify dtype option on import or set low_memory=False.\\n      sf_permits = pd.read_csv(\\"./Building_Permits.csv\\")\\n\\nOutput:\\n\\n<div>\\n<table >\\n  <thead>\\n    <tr>\\n      <th></th>\\n      <th>Permit Number</th>\\n      <th>Permit Type</th>\\n      <th>Permit Type Definition</th>\\n      <th>Permit Creation Date</th>\\n      <th>Block</th>\\n      <th>Lot</th>\\n      <th>Street Number</th>\\n      <th>Street Number Suffix</th>\\n      <th>Street Name</th>\\n      <th>Street Suffix</th>\\n      <th>...</th>\\n      <th>Existing Construction Type</th>\\n      <th>Existing Construction Type Description</th>\\n      <th>Proposed Construction Type</th>\\n      <th>Proposed Construction Type Description</th>\\n      <th>Site Permit</th>\\n      <th>Supervisor District</th>\\n      <th>Neighborhoods - Analysis Boundaries</th>\\n      <th>Zipcode</th>\\n      <th>Location</th>\\n      <th>Record ID</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>201505065519</td>\\n      <td>4</td>\\n      <td>sign - erect</td>\\n      <td>05/06/2015</td>\\n      <td>0326</td>\\n      <td>023</td>\\n      <td>140</td>\\n      <td>NaN</td>\\n      <td>Ellis</td>\\n      <td>St</td>\\n      <td>...</td>\\n      <td>3.0</td>\\n      <td>constr type 3</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Tenderloin</td>\\n      <td>94102.0</td>\\n      <td>(37.785719256680785, -122.40852313194863)</td>\\n      <td>1380611233945</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>201604195146</td>\\n      <td>4</td>\\n      <td>sign - erect</td>\\n      <td>04/19/2016</td>\\n      <td>0306</td>\\n      <td>007</td>\\n      <td>440</td>\\n      <td>NaN</td>\\n      <td>Geary</td>\\n      <td>St</td>\\n      <td>...</td>\\n      <td>3.0</td>\\n      <td>constr type 3</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Tenderloin</td>\\n      <td>94102.0</td>\\n      <td>(37.78733980600732, -122.41063199757738)</td>\\n      <td>1420164406718</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>201605278609</td>\\n      <td>3</td>\\n      <td>additions alterations or repairs</td>\\n      <td>05/27/2016</td>\\n      <td>0595</td>\\n      <td>203</td>\\n      <td>1647</td>\\n      <td>NaN</td>\\n      <td>Pacific</td>\\n      <td>Av</td>\\n      <td>...</td>\\n      <td>1.0</td>\\n      <td>constr type 1</td>\\n      <td>1.0</td>\\n      <td>constr type 1</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Russian Hill</td>\\n      <td>94109.0</td>\\n      <td>(37.7946573324287, -122.42232562979227)</td>\\n      <td>1424856504716</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>201611072166</td>\\n      <td>8</td>\\n      <td>otc alterations permit</td>\\n      <td>11/07/2016</td>\\n      <td>0156</td>\\n      <td>011</td>\\n      <td>1230</td>\\n      <td>NaN</td>\\n      <td>Pacific</td>\\n      <td>Av</td>\\n      <td>...</td>\\n      <td>5.0</td>\\n      <td>wood frame (5)</td>\\n      <td>5.0</td>\\n      <td>wood frame (5)</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Nob Hill</td>\\n      <td>94109.0</td>\\n      <td>(37.79595867909168, -122.41557405519474)</td>\\n      <td>1443574295566</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>201611283529</td>\\n      <td>6</td>\\n      <td>demolitions</td>\\n      <td>11/28/2016</td>\\n      <td>0342</td>\\n      <td>001</td>\\n      <td>950</td>\\n      <td>NaN</td>\\n      <td>Market</td>\\n      <td>St</td>\\n      <td>...</td>\\n      <td>3.0</td>\\n      <td>constr type 3</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>6.0</td>\\n      <td>Tenderloin</td>\\n      <td>94102.0</td>\\n      <td>(37.78315261897309, -122.40950883997789)</td>\\n      <td>144548169992</td>\\n    </tr>\\n  </tbody>\\n</table>\\n<p>5 rows \xd7 43 columns</p>\\n</div>\\n\\n### 3. Matrix Plot\\n\\nThe matrix plot visualizes missing data by representing data points with vertical bars. Each bar shows the presence (white) or absence (black) of data points.\\n\\n```python\\nmsno.matrix(sf_permits)\\n```\\n\\nOutput:\\n![Matrix Plot](./ml-visualization-missing-values-matrix.png)\\n\\n**Why is it important?**\\nThe matrix plot helps identify patterns in the missing data, such as whether missingness occurs at random or follows a specific pattern.\\n\\n### 4. Bar Plot\\n\\nThe bar plot shows the number of non-missing (present) data points for each column.\\n\\n```python\\nmsno.bar(df)\\n```\\n\\nOutput:\\n![Bar Plot](./ml-visualization-missing-values-bar.png)\\n\\n**Why is it important?**\\nThe bar plot provides a quick overview of the completeness of each column, highlighting columns with a high proportion of missing data.\\n\\n### 5. Heatmap\\n\\nThe heatmap shows the correlation of missingness between different columns. A high correlation indicates that the presence of missing data in one column is related to the presence of missing data in another column.\\n\\n```python\\nmsno.heatmap(df)\\n```\\n\\nOutput:\\n![Heatmap](./ml-visualization-missing-values-heatmap.png)\\n\\n**Why is it important?**\\nThe heatmap helps identify relationships in missingness between columns, which can inform decisions on how to handle missing data, such as imputing missing values based on related columns.\\n\\n### 6. Dendrogram\\n\\nThe dendrogram clusters columns based on the similarity of their missing data patterns.\\n\\n```python\\nmsno.dendrogram(df)\\n```\\n\\nOutput:\\n![Dendrogram](./ml-visualization-missing-values-dendrogram.png)\\n\\n**Why is it important?**\\nThe dendrogram helps identify groups of columns with similar missing data patterns, which can be useful for imputation or for understanding the underlying structure of the data.\\n\\n**Conclusion**\\n\\nVisualizing missing data is a crucial step in data preprocessing. It helps understand the extent and pattern of missingness, guiding the choice of strategies for handling missing data. By using tools like `missingno`, you can quickly and effectively visualize and analyze missing data in your datasets.\\n\\nReferences:\\n\\n- [Missingno Documentation](https://github.com/ResidentMario/missingno)\\n- [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)\\n- [NumPy Documentation](https://numpy.org/doc/stable/)\\n- [Mediuam article](https://medium.com/@mahnoorsalman96/checking-for-missing-values-for-machine-learning-bb4c263a6555)"},{"id":"machine-learning/references-and-code-examples","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/references-and-code-examples","source":"@site/blog/machine-learning/2024-06-02-machine-learning-references.md","title":"Machine Learning References and Code Examples","description":"Install library in a existing Jupiter Notebook","date":"2024-06-02T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"references","permalink":"/gerardo-perrucci/blog/tags/references"},{"label":"code examples","permalink":"/gerardo-perrucci/blog/tags/code-examples"}],"readingTime":0.145,"hasTruncateMarker":false,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/references-and-code-examples","title":"Machine Learning References and Code Examples","authors":["me"],"tags":["Machine Learning","references","code examples"],"image":"./ml-handling-missing-values.png"},"unlisted":false,"prevItem":{"title":"Machine Learning Visualization Missing Values","permalink":"/gerardo-perrucci/blog/machine-learning/visualization-missing-values"},"nextItem":{"title":"Machine Learning Handling Missing Values","permalink":"/gerardo-perrucci/blog/machine-learning/learning-handling-missing-values"}},"content":"## Install library in a existing Jupiter Notebook\\n\\n```python\\n# Install a pip package in the current Jupyter kernel\\nimport sys\\n!{sys.executable} -m pip install YOUR_PACKAGE_NAME\\n```\\n\\nRef: https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/"},{"id":"machine-learning/learning-handling-missing-values","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/learning-handling-missing-values","source":"@site/blog/machine-learning/2024-06-01-machine-learning-handling-missing-values.md","title":"Machine Learning Handling Missing Values","description":"Handling missing values is a crucial step in preparing data for machine learning. This tutorial provides examples of how to manage missing values using Python, focusing on the Pandas library. We\'ll import the necessary libraries, read the data, and explore various methods to handle missing values.","date":"2024-06-01T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"Data Preprocessing","permalink":"/gerardo-perrucci/blog/tags/data-preprocessing"},{"label":"numpy","permalink":"/gerardo-perrucci/blog/tags/numpy"},{"label":"pandas","permalink":"/gerardo-perrucci/blog/tags/pandas"}],"readingTime":11.54,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/learning-handling-missing-values","title":"Machine Learning Handling Missing Values","authors":["me"],"tags":["Machine Learning","Data Preprocessing","numpy","pandas"],"image":"./ml-handling-missing-values.png"},"unlisted":false,"prevItem":{"title":"Machine Learning References and Code Examples","permalink":"/gerardo-perrucci/blog/machine-learning/references-and-code-examples"},"nextItem":{"title":"Machine Learning Pandas iloc Cheatsheet","permalink":"/gerardo-perrucci/blog/machine-learning/iloc-cheatsheet"}},"content":"Handling missing values is a crucial step in preparing data for machine learning. This tutorial provides examples of how to manage missing values using Python, focusing on the Pandas library. We\'ll import the necessary libraries, read the data, and explore various methods to handle missing values.\\n\\n![Machine Learning Handling Missing Values](./ml-handling-missing-values.png)\\n\\nYou can check the full code in the [Jupyter Notebook](https://github.com/centrodph/ml/blob/main/data-processing/Machine%20Learning%20Handling%20Missing%20Values.ipynb)\\n\\n\x3c!-- truncate --\x3ex\\n\\n## Importing Libraries\\n\\nWe begin by importing the necessary libraries for our data manipulation and analysis tasks.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\n```\\n\\n- **NumPy**: A fundamental package for scientific computing in Python. It provides support for arrays, matrices, and numerous mathematical functions.\\n- **Pandas**: A powerful data manipulation and analysis library that provides data structures and functions needed to work with structured data seamlessly.\\n\\nReferences:\\n\\n- [NumPy Documentation](https://numpy.org/doc/stable/)\\n- [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)\\n\\n## Reading the Data\\n\\nWe read the CSV file containing the NFL play-by-play data.\\n\\n```python\\ndata = pd.read_csv(\\"./NFLPlayByPlay2009-2017_v4.csv\\")\\n```\\n\\nYou can download the dataset from the [Kaggle website](https://www.kaggle.com/code/alexisbcook/handling-missing-values/data?select=NFL+Play+by+Play+2009-2017+%28v4%29.csv)\\n\\nDuring the import, a warning indicates that some columns have mixed data types. This can be addressed by specifying the `dtype` option or setting `low_memory=False`.\\n\\nOutput:\\n\\n```\\n/tmp/ipykernel_23803/1150844578.py:1: DtypeWarning: Columns (25,51) have mixed types. Specify dtype option on import or set low_memory=False.\\n  data = pd.read_csv(\\"./NFLPlayByPlay2009-2017_v4.csv\\")\\n```\\n\\nReferences:\\n\\n- [Pandas read_csv Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\\n\\n## Inspecting the Data\\n\\nTo get an overview of the data format, we inspect the first few rows of the dataframe.\\n\\n```python\\ndata.head()\\n```\\n\\nOutput:\\n\\n```\\n         Date      GameID  Drive  qtr  down   time  TimeUnder  TimeSecs  PlayTimeDiff  SideofField  ...  yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  Away_WP_post  Win_Prob       WPA  airWPA  yacWPA  Season\\n0  2009-09-10  2009091000      1    1   NaN  15:00         15     3600           0.0          TEN  ...     NaN     0.485675     0.514325     0.546433     0.453567  0.485675  0.060758     NaN     NaN    2009\\n1  2009-09-10  2009091000      1    1   1.0  14:53         15     3593           7.0          PIT  ...  1.146076     0.546433     0.453567     0.551088     0.448912  0.546433  0.004655 -0.032244  0.036899    2009\\n2  2009-09-10  2009091000      1    1   2.0  14:16         15     3556          37.0          PIT  ...     NaN     0.551088     0.448912     0.510793     0.489207  0.551088 -0.040295     NaN     NaN    2009\\n3  2009-09-10  2009091000      1    1   3.0  13:35         14     3515          41.0          PIT  ... -5.031425     0.510793     0.489207     0.461217     0.538783  0.510793 -0.049576  0.106663 -0.156239    2009\\n4  2009-09-10  2009091000      1    1   4.0  13:27         14     3507           8.0          PIT  ...     NaN     0.461217     0.538783     0.558929     0.441071  0.461217  0.097712     NaN     NaN    2009\\n```\\n\\n## Counting Missing Values\\n\\nWe calculate the number of missing values in each column.\\n\\n```python\\nmissing_values_per_column = data.isnull().sum()\\nmissing_values_per_column[0:10] # taking the first ten columns\\n```\\n\\nOutput:\\n\\n```\\nDate                0\\nGameID              0\\nDrive               0\\nqtr                 0\\ndown            61154\\ntime              224\\nTimeUnder           0\\nTimeSecs          224\\nPlayTimeDiff      444\\nSideofField       528\\ndtype: int64\\n```\\n\\n## Total Missing Values\\n\\nTo understand the proportion of missing data, we calculate the total number of cells and the percentage of missing values.\\n\\n```python\\ntotal_cells = np.product(data.shape)\\ntotal_missing = missing_values_per_column.sum()\\n\\nprint(\'total_missing\', total_missing)\\nprint(\'total_cells\', total_cells)\\nprint(\'percentage missing\', (total_missing / total_cells) * 100)\\n```\\n\\nOutput:\\n\\n```\\ntotal_missing 11505187\\ntotal_cells 41584176\\npercentage missing 27.66722370547874\\n```\\n\\n## Counting Non-Missing Values\\n\\nWe can also count the non-missing values in each column.\\n\\n```python\\ndata[:].count()\\n```\\n\\nOutput:\\n\\n```\\nDate        407688\\nGameID      407688\\nDrive       407688\\nqtr         407688\\ndown        346534\\n             ...\\nWin_Prob    382679\\nWPA         402147\\nairWPA      159187\\nyacWPA      158926\\nSeason      407688\\nLength: 102, dtype: int64\\n```\\n\\n## Removing Rows with Missing Values\\n\\nWhile not recommended, one way to handle missing values is to remove rows that contain them.\\n\\n```python\\nremoved_rows_empty_data = data.dropna()\\nprint(removed_rows_empty_data)\\n```\\n\\nOutput:\\n\\n```\\nEmpty DataFrame\\nColumns: [Date, GameID, Drive, qtr, down, time, TimeUnder, TimeSecs, PlayTimeDiff, SideofField, yrdln, yrdline100, ydstogo, ydsnet, GoalToGo, FirstDown, posteam, DefensiveTeam, desc, PlayAttempted, Yards.Gained, sp, Touchdown, ExPointResult, TwoPointConv, DefTwoPoint, Safety, Onsidekick, PuntResult, PlayType, Passer, Passer_ID, PassAttempt, PassOutcome, PassLength, AirYards, YardsAfterCatch, QBHit, PassLocation, InterceptionThrown, Interceptor, Rusher, Rusher_ID, RushAttempt, RunLocation, RunGap, Receiver, Receiver_ID, Reception, ReturnResult, Returner, BlockingPlayer, Tackler1, Tackler2, FieldGoalResult, FieldGoalDistance, Fumble, RecFumbTeam, RecFumbPlayer, Sack, Challenge.Replay, ChalReplayResult, Accepted.Penalty, PenalizedTeam, PenaltyType, PenalizedPlayer, Penalty.Yards, PosTeamScore, DefTeamScore, ScoreDiff, AbsScoreDiff, HomeTeam, AwayTeam, Timeout_Indicator, Timeout_Team, posteam_timeouts_pre, HomeTimeouts_Remaining_Pre, AwayTimeouts_Remaining_Pre, HomeTimeouts_Remaining_Post, AwayTimeouts_Remaining_Post, No_Score_Prob, Opp_Field_Goal_Prob, Opp_Safety_Prob, Opp_Touchdown_Prob, Field_Goal_Prob, Safety_Prob, Touchdown_Prob, ExPoint_Prob, TwoPoint_Prob, ExpPts, EPA, airEPA, yacEPA, Home_WP_pre, Away_WP_pre, Home_WP_post, Away_WP_post, Win_Prob, WPA, airWPA, yacWPA, ...]\\nIndex: []\\n[0 rows x 102 columns]\\n```\\n\\n### Removing Columns with Missing Values\\n\\nA more common approach is to remove columns that contain missing values.\\n\\n```python\\nremoved_columns_empty_data = data.dropna(axis=1)\\nprint(removed_columns_empty_data)\\n```\\n\\nOutput:\\n\\n```\\n            Date      GameID  Drive  qtr  TimeUnder  ydstogo  ydsnet  PlayAttempted  Yards.Gained  sp  ...  AwayTeam  Timeout_Indicator  posteam_timeouts_pre HomeTimeouts_Remaining_Pre  AwayTimeouts_Remaining_Pre  HomeTimeouts_Remaining_Post  AwayTimeouts_Remaining_Post  ExPoint_Prob  TwoPoint_Prob  Season\\n0      2009-09-10  2009091000      1    1         15        0       0               1            39   0  ...       TEN                  0                        3                          3                          3                            3                            3           0.0            0.0    2009\\n1      2009-09-10  2009091000      1    1         15       10       5\\n\\n 1             5   0  ...       TEN                  0                        3                          3                          3                            3                            3           0.0            0.0    2009\\n2      2009-09-10  2009091000      1    1         15        5       2               1            -3   0  ...       TEN                  0                        3                          3                          3                            3                            3           0.0            0.0    2009\\n3      2009-09-10  2009091000      1    1         14        8       2               1             0   0  ...       TEN                  0                        3                          3                          3                            3                            3           0.0            0.0    2009\\n4      2009-09-10  2009091000      1    1         14        8       2               1             0   0  ...       TEN                  0                        3                          3                          3                            3                            3           0.0            0.0    2009\\n...           ...         ...    ...  ...        ...      ...     ...             ...           ...  ..  ...       ...                ...                      ...                        ...                          ...                          ...                          ...           ...            ...     ...\\n407683 2017-12-31  2017123101     29    4          1        0      -4               1             0   0  ...       CIN                  1                        0                          3                          0                            2                            0           0.0            0.0    2017\\n407684 2017-12-31  2017123101     29    4          1       14      -4               1             0   0  ...       CIN                  0                        2                          2                          0                            2                            0           0.0            0.0    2017\\n407685 2017-12-31  2017123101     29    4          1       14       9               1            13   0  ...       CIN                  0                        2                          2                          0                            2                            0           0.0            0.0    2017\\n407686 2017-12-31  2017123101     30    4          1       10      -1               1            -1   0  ...       CIN                  0                        0                          2                          0                            2                            0           0.0            0.0    2017\\n407687 2017-12-31  2017123101     30    4          0        0      -1               1             0   0  ...       CIN                  0                        0                          2                          0                            2                            0           0.0            0.0    2017\\n[407688 rows x 37 columns]\\n```\\n\\nWe then calculate the impact of this operation by comparing the number of columns before and after.\\n\\n```python\\nprint(\\"original columns: %d \\\\n\\" % data.shape[1])\\nprint(\\"cleaned columns: %d \\\\n\\" % removed_columns_empty_data.shape[1])\\n```\\n\\nOutput:\\n\\n```\\noriginal columns: 102\\ncleaned columns: 37\\n```\\n\\n## Subsetting the Data\\n\\nTo focus on a smaller portion of the dataset, we can create a subset.\\n\\n```python\\nsubset_nfl_data = data.loc[:, \'EPA\':\'Season\'].head()\\nsubset_nfl_data\\n```\\n\\nOutput:\\n\\n```\\n         EPA    airEPA    yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season\\n0  2.014474       NaN       NaN     0.485675     0.514325     0.546433     0.453567  0.485675  0.060758       NaN       NaN    2009\\n1  0.077907 -1.068169  1.146076     0.546433     0.453567     0.551088     0.448912  0.546433  0.004655 -0.032244  0.036899    2009\\n2 -1.402760       NaN       NaN     0.551088     0.448912     0.510793     0.489207  0.551088 -0.040295       NaN       NaN    2009\\n3 -1.712583  3.318841 -5.031425     0.510793     0.489207     0.461217     0.538783  0.510793 -0.049576  0.106663 -0.156239    2009\\n4  2.097796       NaN       NaN     0.461217     0.538783     0.558929     0.441071  0.461217  0.097712       NaN       NaN    2009\\n```\\n\\n## Basic Filling of Missing Values\\n\\nA straightforward method for handling missing values is to fill them with a specific value, such as zero.\\n\\n```python\\nfilled_basic_data = data.fillna(0)\\nfilled_basic_data.head()\\n```\\n\\nOutput:\\n\\n```\\n         Date      GameID  Drive  qtr  down   time  TimeUnder  TimeSecs  PlayTimeDiff  SideofField  ...    yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season\\n0  2009-09-10  2009091000      1    1   0.0  15:00         15     3600           0.0          TEN  ...  0.000000     0.485675     0.514325     0.546433     0.453567  0.485675  0.060758  0.000000  0.000000    2009\\n1  2009-09-10  2009091000      1    1   1.0  14:53         15     3593           7.0          PIT  ...  1.146076     0.546433     0.453567     0.551088     0.448912  0.546433  0.004655 -0.032244  0.036899    2009\\n2  2009-09-10  2009091000      1    1   2.0  14:16         15     3556          37.0          PIT  ...  0.000000     0.551088     0.448912     0.510793     0.489207  0.551088 -0.040295  0.000000  0.000000    2009\\n3  2009-09-10  2009091000      1    1   3.0  13:35         14     3515          41.0          PIT  ... -5.031425     0.510793     0.489207     0.461217     0.538783  0.510793 -0.049576  0.106663 -0.156239    2009\\n4  2009-09-10  2009091000      1    1   4.0  13:27         14     3507           8.0          PIT  ...  0.000000     0.461217     0.538783     0.558929     0.441071  0.461217  0.097712  0.000000  0.000000    2009\\n```\\n\\n## Column-Based Filling\\n\\nAnother approach is to fill missing values based on the next valid observation in the column.\\n\\n```python\\ncolumn_based_fill = data.bfill(axis=0).fillna(0)\\ncolumn_based_fill.head()\\n```\\n\\nOutput:\\n\\n```\\n         Date      GameID  Drive  qtr  down   time  TimeUnder  TimeSecs  PlayTimeDiff  SideofField  ...    yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season\\n0  2009-09-10  2009091000      1    1   1.0  15:00         15     3600           0.0          TEN  ...  1.146076     0.485675     0.514325     0.546433\\n\\n 0.453567  0.485675  0.060758 -0.032244  0.036899    2009\\n1  2009-09-10  2009091000      1    1   1.0  14:53         15     3593           7.0          PIT  ...  1.146076     0.546433     0.453567     0.551088      0.448912  0.546433  0.004655 -0.032244  0.036899    2009\\n2  2009-09-10  2009091000      1    1   2.0  14:16         15     3556          37.0          PIT  ... -5.031425     0.551088     0.448912     0.510793      0.489207  0.551088 -0.040295  0.106663 -0.156239    2009\\n3  2009-09-10  2009091000      1    1   3.0  13:35         14     3515          41.0          PIT  ... -5.031425     0.510793     0.489207     0.461217      0.538783  0.510793 -0.049576  0.106663 -0.156239    2009\\n4  2009-09-10  2009091000      1    1   4.0  13:27         14     3507           8.0          PIT  ...  0.163935     0.461217     0.538783     0.558929      0.441071  0.461217  0.097712 -0.010456  0.006029    2009\\n```\\n\\nReferences:\\n\\n- [Pandas Handling Missing Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\\n\\nThese steps provide a comprehensive guide to identifying and handling missing values in a dataset, ensuring the data is ready for analysis and modeling. Each method has its pros and cons, and the choice of method depends on the specific context and requirements of your analysis.\\n\\n## Extra exercise from kaggle\\n\\n**This notebook is an exercise in the [Data Cleaning](https://www.kaggle.com/learn/data-cleaning) course. You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/handling-missing-values).**\\n\\n---\\n\\nIn this exercise, you\'ll apply what you learned in the **Handling missing values** tutorial.\\n\\n# 1) Take a first look at the data\\n\\nRun the next code cell to load in the libraries and dataset you\'ll use to complete the exercise.\\n\\n```python\\n# modules we\'ll use\\nimport pandas as pd\\nimport numpy as np\\n\\n# read in all our data\\nsf_permits = pd.read_csv(\\"./Building_Permits.csv\\")\\n\\n# set seed for reproducibility\\nnp.random.seed(0)\\n```\\n\\n    /tmp/ipykernel_33/3534875831.py:6: DtypeWarning: Columns (22,32) have mixed types. Specify dtype option on import or set low_memory=False.\\n      sf_permits = pd.read_csv(\\"../input/building-permit-applications-data/Building_Permits.csv\\")\\n\\nYou can download the dataset from the [Kaggle website](https://www.kaggle.com/code/centrodph/exercise-handling-missing-values).\\n\\nUse the code cell below to print the first five rows of the `sf_permits` DataFrame.\\n\\n```python\\n# TODO: Your code here!\\nsf_permits.head()\\n```\\n\\n<div>\\n<table>\\n  <thead>\\n    <tr>\\n      <th></th>\\n      <th>Permit Number</th>\\n      <th>Permit Type</th>\\n      <th>Permit Type Definition</th>\\n      <th>Permit Creation Date</th>\\n      <th>Block</th>\\n      <th>Lot</th>\\n      <th>Street Number</th>\\n      <th>Street Number Suffix</th>\\n      <th>Street Name</th>\\n      <th>Street Suffix</th>\\n      <th>...</th>\\n      <th>Existing Construction Type</th>\\n      <th>Existing Construction Type Description</th>\\n      <th>Proposed Construction Type</th>\\n      <th>Proposed Construction Type Description</th>\\n      <th>Site Permit</th>\\n      <th>Supervisor District</th>\\n      <th>Neighborhoods - Analysis Boundaries</th>\\n      <th>Zipcode</th>\\n      <th>Location</th>\\n      <th>Record ID</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>201505065519</td>\\n      <td>4</td>\\n      <td>sign - erect</td>\\n      <td>05/06/2015</td>\\n      <td>0326</td>\\n      <td>023</td>\\n      <td>140</td>\\n      <td>NaN</td>\\n      <td>Ellis</td>\\n      <td>St</td>\\n      <td>...</td>\\n      <td>3.0</td>\\n      <td>constr type 3</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Tenderloin</td>\\n      <td>94102.0</td>\\n      <td>(37.785719256680785, -122.40852313194863)</td>\\n      <td>1380611233945</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>201604195146</td>\\n      <td>4</td>\\n      <td>sign - erect</td>\\n      <td>04/19/2016</td>\\n      <td>0306</td>\\n      <td>007</td>\\n      <td>440</td>\\n      <td>NaN</td>\\n      <td>Geary</td>\\n      <td>St</td>\\n      <td>...</td>\\n      <td>3.0</td>\\n      <td>constr type 3</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Tenderloin</td>\\n      <td>94102.0</td>\\n      <td>(37.78733980600732, -122.41063199757738)</td>\\n      <td>1420164406718</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>201605278609</td>\\n      <td>3</td>\\n      <td>additions alterations or repairs</td>\\n      <td>05/27/2016</td>\\n      <td>0595</td>\\n      <td>203</td>\\n      <td>1647</td>\\n      <td>NaN</td>\\n      <td>Pacific</td>\\n      <td>Av</td>\\n      <td>...</td>\\n      <td>1.0</td>\\n      <td>constr type 1</td>\\n      <td>1.0</td>\\n      <td>constr type 1</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Russian Hill</td>\\n      <td>94109.0</td>\\n      <td>(37.7946573324287, -122.42232562979227)</td>\\n      <td>1424856504716</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>201611072166</td>\\n      <td>8</td>\\n      <td>otc alterations permit</td>\\n      <td>11/07/2016</td>\\n      <td>0156</td>\\n      <td>011</td>\\n      <td>1230</td>\\n      <td>NaN</td>\\n      <td>Pacific</td>\\n      <td>Av</td>\\n      <td>...</td>\\n      <td>5.0</td>\\n      <td>wood frame (5)</td>\\n      <td>5.0</td>\\n      <td>wood frame (5)</td>\\n      <td>NaN</td>\\n      <td>3.0</td>\\n      <td>Nob Hill</td>\\n      <td>94109.0</td>\\n      <td>(37.79595867909168, -122.41557405519474)</td>\\n      <td>1443574295566</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>201611283529</td>\\n      <td>6</td>\\n      <td>demolitions</td>\\n      <td>11/28/2016</td>\\n      <td>0342</td>\\n      <td>001</td>\\n      <td>950</td>\\n      <td>NaN</td>\\n      <td>Market</td>\\n      <td>St</td>\\n      <td>...</td>\\n      <td>3.0</td>\\n      <td>constr type 3</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>6.0</td>\\n      <td>Tenderloin</td>\\n      <td>94102.0</td>\\n      <td>(37.78315261897309, -122.40950883997789)</td>\\n      <td>144548169992</td>\\n    </tr>\\n  </tbody>\\n</table>\\n<p>5 rows \xd7 43 columns</p>\\n</div>\\n\\n<span >Correct:</span>\\n\\nThe first five rows of the data does show that several columns have missing values. You can see this in the \\"Street Number Suffix\\", \\"Proposed Construction Type\\" and \\"Site Permit\\" columns, among others.\\n\\n# 2) How many missing data points do we have?\\n\\nWhat percentage of the values in the dataset are missing? Your answer should be a number between 0 and 100. (If 1/4 of the values in the dataset are missing, the answer is 25.)\\n\\n```python\\n# TODO: Your code here!\\ntotal_cells = np.product(sf_permits.shape)\\nmissing_values_count = sf_permits.isnull().sum();\\n\\ntotal_missing_cells = missing_values_count.sum();\\n\\n# print(\\"shape\\", sf_permits.shape)\\nprint(\'total cells\', total_cells)\\n# print(\'total missing per column\', missing_values_count)\\nprint(\'total missing cells\', total_missing_cells)\\n\\npercent_missing = (total_missing_cells / total_cells) * 100\\n\\nprint(\'percent missing\', percent_missing)\\n\\n```\\n\\n    total cells 8552700\\n    total missing cells 2245941\\n    percent missing 26.26002315058403\\n\\n<span >Correct</span>\\n\\n# 3) Figure out why the data is missing\\n\\nLook at the columns **\\"Street Number Suffix\\"** and **\\"Zipcode\\"** from the [San Francisco Building Permits dataset](https://www.kaggle.com/aparnashastry/building-permit-applications-data). Both of these contain missing values.\\n\\n- Which, if either, are missing because they don\'t exist?\\n- Which, if either, are missing because they weren\'t recorded?\\n\\nOnce you have an answer, run the code cell below.\\n\\n<span >Correct:</span>\\n\\nIf a value in the \\"Street Number Suffix\\" column is missing, it is likely because it does not exist. If a value in the \\"Zipcode\\" column is missing, it was not recorded.\\n\\n# 4) Drop missing values: rows\\n\\nIf you removed all of the rows of `sf_permits` with missing values, how many rows are left?\\n\\n**Note**: Do not change the value of `sf_permits` when checking this.\\n\\n```python\\n# TODO: Your code here!\\ntotal_rows = sf_permits.shape[0]\\ntotal_rows_after_drop= sf_permits.dropna().shape[0]\\n\\nprint(total_rows)\\nprint(total_rows_after_drop) # no rows\\n```\\n\\n    198900\\n    0\\n\\n<span >Correct:</span>\\n\\nThere are no rows remaining in the dataset!\\n\\n# 5) Drop missing values: columns\\n\\nNow try removing all the columns with empty values.\\n\\n- Create a new DataFrame called `sf_permits_with_na_dropped` that has all of the columns with empty values removed.\\n- How many columns were removed from the original `sf_permits` DataFrame? Use this number to set the value of the `dropped_columns` variable below.\\n\\n```\\nsf_permits_with_na_dropped = sf_permits.dropna(axis=1)\\n\\ndropped_columns = sf_permits.shape[1] - sf_permits_with_na_dropped.shape[1]\\n```\\n\\n# 6) Fill in missing values automatically\\n\\nTry replacing all the NaN\'s in the `sf_permits` data with the one that comes directly after it and then replacing any remaining NaN\'s with 0. Set the result to a new DataFrame `sf_permits_with_na_imputed`.\\n\\n```\\nsf_permits_with_na_imputed = sf_permits.bfill(axis=0).fillna(0)\\n```\\n\\n# More practice\\n\\n- Check out [this noteboook](https://www.kaggle.com/alexisbcook/missing-values) on handling missing values using scikit-learn\'s imputer.\\n- Look back at the \\"Zipcode\\" column in the `sf_permits` dataset, which has some missing values. How would you go about figuring out what the actual zipcode of each address should be? (You might try using another dataset. You can search for datasets about San Fransisco on the [Datasets listing](https://www.kaggle.com/datasets).)"},{"id":"machine-learning/iloc-cheatsheet","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/iloc-cheatsheet","source":"@site/blog/machine-learning/2024-05-29-machine-learning-iloc.md","title":"Machine Learning Pandas iloc Cheatsheet","description":"The iloc indexer in pandas is a powerful tool for data selection, slicing, and manipulation, essential for preparing datasets for machine learning tasks. Here\'s a comprehensive guide to help you master iloc.","date":"2024-05-29T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"Data Preprocessing","permalink":"/gerardo-perrucci/blog/tags/data-preprocessing"},{"label":"iloc","permalink":"/gerardo-perrucci/blog/tags/iloc"},{"label":"pandas","permalink":"/gerardo-perrucci/blog/tags/pandas"}],"readingTime":5.65,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/iloc-cheatsheet","title":"Machine Learning Pandas iloc Cheatsheet","authors":["me"],"tags":["Machine Learning","Data Preprocessing","iloc","pandas"],"image":"./ml-pandas-iloc.png"},"unlisted":false,"prevItem":{"title":"Machine Learning Handling Missing Values","permalink":"/gerardo-perrucci/blog/machine-learning/learning-handling-missing-values"},"nextItem":{"title":"Machine Learning: Feature Scaling","permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling"}},"content":"The `iloc` indexer in pandas is a powerful tool for data selection, slicing, and manipulation, essential for preparing datasets for machine learning tasks. Here\'s a comprehensive guide to help you master `iloc`.\\n\\nYou can download the .ipynb file from [here](https://github.com/centrodph/ml/blob/main/data-processing/Pandas%20iloc%20Cheatsheet%20for%20Machine%20Learning.ipynb)\\n\\n![Machine Learning Pandas iloc Cheatsheet](./ml-pandas-iloc.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Table of Contents\\n\\n1. **Introduction to `iloc`**\\n2. **Basic Usage**\\n   - Selecting Rows\\n   - Selecting Columns\\n3. **Advanced Indexing**\\n   - Slicing Rows and Columns\\n   - Selecting Specific Rows and Columns\\n4. **Conditional Selection**\\n5. **Modifying Data**\\n6. **Practical Machine Learning Examples**\\n   - Splitting Data into Features and Target\\n   - Handling Missing Data\\n   - Data Normalization\\n7. **Oficial documentation**\\n8. **Tutorial Videos**\\n\\n## 1. Introduction to `iloc`\\n\\nThe `iloc` indexer is used for integer-location based indexing for selection by position. It is one of the primary indexers for Pandas data structures.\\n\\n```python\\nimport pandas as pd\\n\\n# Sample DataFrame\\ndata = {\\n    \'A\': [1, 2, 3, 4],\\n    \'B\': [5, 6, 7, 8],\\n    \'C\': [9, 10, 11, 12],\\n    \'D\': [13, 14, 15, 16]\\n}\\ndf = pd.DataFrame(data)\\nprint(df)\\n\\n```\\n\\n```\\n       A  B   C   D\\n    0  1  5   9  13\\n    1  2  6  10  14\\n    2  3  7  11  15\\n    3  4  8  12  16\\n```\\n\\n## 2. Basic Usage\\n\\n### Selecting Rows\\n\\nTo select rows using `iloc`, you specify the row index.\\n\\n```python\\n# Select the first row\\nprint(df.iloc[0])\\n```\\n\\n```\\n    A     1\\n    B     5\\n    C     9\\n    D    13\\n    Name: 0, dtype: int64\\n```\\n\\n```python\\n# Select the first three rows\\nprint(df.iloc[:3])\\n```\\n\\n```\\n       A  B   C   D\\n    0  1  5   9  13\\n    1  2  6  10  14\\n    2  3  7  11  15\\n```\\n\\n### Selecting Columns\\n\\nTo select columns, you specify the column index.\\n\\n```python\\n# Select the first column\\nprint(df.iloc[:, 0])\\n\\n```\\n\\n```\\n    0    1\\n    1    2\\n    2    3\\n    3    4\\n    Name: A, dtype: int64\\n```\\n\\n```python\\n# Select the first two columns\\nprint(df.iloc[:, :2])\\n```\\n\\n```\\n       A  B\\n    0  1  5\\n    1  2  6\\n    2  3  7\\n    3  4  8\\n```\\n\\n## 3. Advanced Indexing\\n\\n### Slicing Rows and Columns\\n\\nYou can slice both rows and columns simultaneously.\\n\\n```python\\n# Select the first two rows and the first two columns\\nprint(df.iloc[:2, :2])\\n```\\n\\n```\\n       A  B\\n    0  1  5\\n    1  2  6\\n```\\n\\n### Selecting Specific Rows and Columns\\n\\nSpecify exact row and column indices.\\n\\n```python\\n# Select the first and third rows and the second and fourth columns\\nprint(df.iloc[[0, 2], [1, 3]])\\n```\\n\\n```\\n       B   D\\n    0  5  13\\n    2  7  15\\n```\\n\\n## 4. Conditional Selection\\n\\nUsing `iloc` in combination with conditions.\\n\\n```python\\n# Example DataFrame\\ndf_cond = pd.DataFrame({\\n    \'A\': [1, 2, 3, 4, 5],\\n    \'B\': [10, 20, 30, 40, 50],\\n    \'C\': [100, 200, 300, 400, 500]\\n})\\n\\n# Condition to select rows where column \'A\' values are greater than 2\\nprint(df_cond[df_cond[\'A\'] > 2].iloc[:, [0, 2]])  # Select columns \'A\' and \'C\'\\n```\\n\\n```\\n       A    C\\n    2  3  300\\n    3  4  400\\n    4  5  500\\n```\\n\\n## 5. Modifying Data\\n\\nYou can use `iloc` to modify specific parts of the DataFrame.\\n\\n```python\\n# Set the value of the first cell to 0\\ndf.iloc[0, 0] = 0\\nprint(df)\\n\\n# Set the values of the first column to 0\\ndf.iloc[:, 0] = 0\\nprint(df)\\n```\\n\\n```\\n       A  B   C   D\\n    0  0  5   9  13\\n    1  2  6  10  14\\n    2  3  7  11  15\\n    3  4  8  12  16\\n       A  B   C   D\\n    0  0  5   9  13\\n    1  0  6  10  14\\n    2  0  7  11  15\\n    3  0  8  12  16\\n```\\n\\n## 6. Practical Machine Learning Examples\\n\\n### Splitting Data into Features and Target\\n\\nSeparating features (X) and target (y) is a common task.\\n\\n```python\\n# Sample DataFrame with a target column\\ndf_ml = pd.DataFrame({\\n    \'Feature1\': [1, 2, 3, 4, 5],\\n    \'Feature2\': [10, 20, 30, 40, 50],\\n    \'Target\': [0, 1, 0, 1, 0]\\n})\\n\\n# Features (all rows, all columns except the last one)\\nX = df_ml.iloc[:, :-1]\\n\\n# Target (all rows, last column)\\ny = df_ml.iloc[:, -1]\\n\\nprint(\\"Features:\\\\n\\", X)\\nprint(\\"Target:\\\\n\\", y)\\n```\\n\\n```\\n    Features:\\n        Feature1  Feature2\\n    0         1        10\\n    1         2        20\\n    2         3        30\\n    3         4        40\\n    4         5        50\\n    Target:\\n     0    0\\n    1    1\\n    2    0\\n    3    1\\n    4    0\\n    Name: Target, dtype: int64\\n```\\n\\n### Handling Missing Data\\n\\nUsing `iloc` to handle missing data by selecting specific parts of the DataFrame.\\n\\n```python\\n# Sample DataFrame with missing values\\ndf_missing = pd.DataFrame({\\n    \'A\': [1, 2, None, 4],\\n    \'B\': [5, None, 7, 8],\\n    \'C\': [None, 10, 11, 12]\\n})\\n\\n# Fill missing values in the first two columns with 0\\ndf_missing.iloc[:, :2] = df_missing.iloc[:, :2].fillna(0)\\nprint(df_missing)\\n```\\n\\n```\\n         A    B     C\\n    0  1.0  5.0   NaN\\n    1  2.0  0.0  10.0\\n    2  0.0  7.0  11.0\\n    3  4.0  8.0  12.0\\n```\\n\\n### Data Normalization\\n\\nUsing `iloc` to normalize data.\\n\\n```python\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# Sample DataFrame for normalization\\ndf_norm = pd.DataFrame({\\n    \'Feature1\': [1, 2, 3, 4, 5],\\n    \'Feature2\': [10, 20, 30, 40, 50]\\n})\\n\\nscaler = MinMaxScaler()\\n\\n# Normalize the first two columns\\ndf_norm.iloc[:, :2] = scaler.fit_transform(df_norm.iloc[:, :2])\\nprint(df_norm)\\n```\\n\\n```\\n       Feature1  Feature2\\n    0      0.00      0.00\\n    1      0.25      0.25\\n    2      0.50      0.50\\n    3      0.75      0.75\\n    4      1.00      1.00\\n```\\n\\nCertainly! Here are some references to official documentation and YouTube videos that can help you learn more about using the `iloc` indexer in pandas for machine learning:\\n\\n## Official Documentation\\n\\n1. **Pandas Documentation on Indexing and Selecting Data:**\\n\\n   - [Pandas Official Documentation - Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\\n   - This section of the pandas documentation provides comprehensive details on various indexing methods, including `iloc`.\\n\\n2. **Pandas API Reference for `iloc`:**\\n   - [Pandas API Reference - iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)\\n   - This page contains detailed information about the `iloc` property and its usage.\\n\\n## Tutorial Videos\\n\\n1. **Corey Schafer - Python Pandas DataFrame Tutorial:**\\n\\n   - [Selecting Rows and Columns from a Pandas DataFrame](https://www.youtube.com/watch?v=ZyhVh-qRZPA&list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS&ab_channel=CoreySchafer)\\n   - This playlist covers various methods to select rows and columns in pandas DataFrames, including the use of `iloc`.\\n\\n2. **Data School - How do I select a subset of a DataFrame:**\\n\\n   - [Data School - Pandas iloc](https://www.youtube.com/watch?v=xvpNA7bC8cs)\\n   - Data School provides an in-depth tutorial on selecting subsets of DataFrames using `iloc`.\\n\\n3. **Getting Started with Data Analysis:**\\n\\n   - [Pandas DataFrames in Python](https://www.youtube.com/watch?v=ZyhVh-qRZPA)\\n   - This video explains the basics of pandas DataFrames and covers various indexing techniques including `iloc`.\\n\\n4. **Pandas Tutorial:**\\n   - [Pandas Tutorial (Data Analysis with Python)](https://www.youtube.com/watch?v=vmEHCJofslg)\\n   - A comprehensive tutorial on pandas covering many aspects including data selection and manipulation using `iloc`.\\n\\nThese resources should provide you with a strong foundation for understanding and utilizing `iloc` in pandas for your machine learning projects.\\n\\n## Conclusion\\n\\nThe `iloc` indexer is a versatile and powerful tool for data manipulation in pandas, especially useful in the preprocessing stages of machine learning. Mastering `iloc` allows for efficient and precise data selection and modification, essential for building robust machine learning models."},{"id":"machine-learning/machine-learning-feature-scaling","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling","source":"@site/blog/machine-learning/2024-05-27-machine-learning-feature-scaling.md","title":"Machine Learning: Feature Scaling","description":"Imagine you\'re a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual\'s absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.","date":"2024-05-27T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"Data Preprocessing","permalink":"/gerardo-perrucci/blog/tags/data-preprocessing"},{"label":"Feature Scaling","permalink":"/gerardo-perrucci/blog/tags/feature-scaling"}],"readingTime":3.165,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/machine-learning-feature-scaling","title":"Machine Learning: Feature Scaling","authors":["me"],"tags":["Machine Learning","Data Preprocessing","Feature Scaling"],"image":"./ml-feature-scaling.webp"},"unlisted":false,"prevItem":{"title":"Machine Learning Pandas iloc Cheatsheet","permalink":"/gerardo-perrucci/blog/machine-learning/iloc-cheatsheet"},"nextItem":{"title":"Machine Learning Process: A Comprehensive Guide","permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-process"}},"content":"Imagine you\'re a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual\'s absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.\\n\\n![Machine Learning Feature Scaling Source: someka.net](./ml-feature-scaling.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n**Feature scaling** is a data pre-processing technique that addresses this issue. It essentially standardizes the range of features in your dataset, ensuring all features contribute equally during model training. Let\'s delve deeper into why and how this works.\\n\\n### Why Scale?\\n\\n- **Fair Play for All Features:** Features with larger values can overshadow those with smaller ones, even if the smaller ones hold valuable information. Scaling creates a level playing field.\\n- **Distance Matters:** Many machine learning algorithms rely on calculating distances between data points. Feature scaling ensures these distances accurately reflect the underlying relationships.\\n- **Faster & More Efficient Learning:** By putting features on a similar scale, the learning algorithm can converge (find an optimal solution) faster and more efficiently.\\n\\n### Normalization vs. Standardization: Two Sides of the Scaling Coin\\n\\nNormalization and standardization are two common feature scaling techniques, and the terms are sometimes used interchangeably. However, there\'s a subtle difference:\\n\\n- **Normalization:** This technique scales features to a specific range, typically between 0 and 1 (Min-Max Scaling) or -1 and 1. It\'s useful when you know the data distribution or want to bound values within a specific range.\\n\\n  - Formula:\\n\\n    ```\\n    X_scaled = (X - min(X)) / (max(X) - min(X))\\n    ```\\n\\n    Here,\\n    _ X_scaled is the normalized feature\\n    _ X is the original feature value\\n    _ min(X) is the minimum value in the feature\\n    _ max(X) is the maximum value in the feature\\n\\n- **Standardization:** This technique transforms features to have a mean of 0 and a standard deviation of 1 (Z-score normalization). It assumes a Gaussian (bell-shaped) distribution for the data and emphasizes outliers more than normalization.\\n\\n  - Formula:\\n\\n    ```\\n    X_scaled = (X - mean(X)) / std(X)\\n    ```\\n\\n    Here,\\n    _ X_scaled is the standardized feature\\n    _ X is the original feature value\\n    _ mean(X) is the average of all values in the feature\\n    _ std(X) is the standard deviation of the feature\\n\\n**Choosing the Right Technique:**\\n\\nThe best technique depends on your data and the specific algorithm you\'re using. Here\'s a general guideline:\\n\\n- Use Min-Max scaling if the data distribution is unknown or outliers are not a concern.\\n- Use standardization (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.\\n\\n**Examples:**\\n\\nImagine a dataset with two features: house price (in millions) and distance from a school (in meters). Without scaling, the massive price range would overpower the distance information. Scaling levels the field, allowing the model to learn from both features effectively.\\n\\n**Further Learning:**\\n\\n- [Feature Scaling and Why Does Machine Learning Need It](https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048)\\n- [Feature Engineering: Scaling, Normalization, and Standardization](https://www.geeksforgeeks.org/ml-feature-scaling-part-2/)\\n- [Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\\n\\nRemember, feature scaling is a crucial step in building robust and accurate machine learning models. By ensuring all features are on the same page, you can empower your models to learn from your data more effectively.\\n\\n**Choosing the Right Technique:**\\n\\nThe best technique depends on your data and the specific algorithm you\'re using. Here\'s a general guideline:\\n\\n- Use `normalization` scaling if the data distribution is unknown or outliers are not a concern.\\n- Use `standardization` (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.\\n\\nI hope this addition clarifies the concepts of normalization and standardization with their respective formulas!"},{"id":"machine-learning/machine-learning-process","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-process","source":"@site/blog/machine-learning/2024-05-27-machine-learning-process.md","title":"Machine Learning Process: A Comprehensive Guide","description":"Machine learning (ML) has become a cornerstone of modern technology, driving advancements in various fields such as healthcare, finance, and transportation. To build effective ML models, it\'s essential to understand the three main steps in the machine learning process: Data Preprocessing, Modeling, and Evaluation. This article breaks down these steps, detailing the sub-steps involved and providing references for further reading and understanding.","date":"2024-05-27T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"Data Preprocessing","permalink":"/gerardo-perrucci/blog/tags/data-preprocessing"},{"label":"Modeling","permalink":"/gerardo-perrucci/blog/tags/modeling"},{"label":"Evaluation","permalink":"/gerardo-perrucci/blog/tags/evaluation"}],"readingTime":2.115,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/machine-learning-process","title":"Machine Learning Process: A Comprehensive Guide","authors":["me"],"tags":["Machine Learning","Data Preprocessing","Modeling","Evaluation"],"image":"./ml-process.png"},"unlisted":false,"prevItem":{"title":"Machine Learning: Feature Scaling","permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling"},"nextItem":{"title":"Machine Learning Environment: Python, R, RStudio, and Colab","permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-tools"}},"content":"Machine learning (ML) has become a cornerstone of modern technology, driving advancements in various fields such as healthcare, finance, and transportation. To build effective ML models, it\'s essential to understand the three main steps in the machine learning process: Data Preprocessing, Modeling, and Evaluation. This article breaks down these steps, detailing the sub-steps involved and providing references for further reading and understanding.\\n\\n![Machine Learning Process](./ml-process.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Data Preprocessing\\n\\nData preprocessing is the first and arguably the most crucial step in the machine learning pipeline. This step ensures that the data is clean, consistent, and suitable for the modeling process.\\n\\n### Sub-steps:\\n\\n1. **Data Collection**: Gathering relevant data from various sources. This can include databases, APIs, and web scraping.\\n2. **Data Cleaning**: Removing or correcting any inaccuracies in the data, such as missing values, outliers, and duplicates.\\n3. **Data Transformation**: Converting data into a suitable format for analysis, which might involve normalization, standardization, or encoding categorical variables.\\n4. **Data Splitting**: Dividing the data into training, validation, and test sets to evaluate the model\'s performance.\\n\\n### References:\\n\\n- [Scikit-Learn Documentation on Data Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\\n- [Kaggle Data Preprocessing Tutorial](https://www.kaggle.com/learn/data-cleaning)\\n\\n## 2. Modeling\\n\\nOnce the data is preprocessed, the next step is to build and train the machine learning model. This involves selecting the appropriate algorithm and fine-tuning it to achieve the best performance.\\n\\n### Sub-steps:\\n\\n1. **Algorithm Selection**: Choosing a machine learning algorithm based on the problem type (e.g., regression, classification, clustering).\\n2. **Model Training**: Feeding the training data into the algorithm to learn the underlying patterns and relationships.\\n3. **Hyperparameter Tuning**: Adjusting the algorithm\'s parameters to optimize performance. This can be done using techniques like grid search or random search.\\n\\n### References:\\n\\n- [Scikit-Learn Documentation on Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html)\\n- [TensorFlow Model Training Guide](https://www.tensorflow.org/guide/keras/train_and_evaluate)\\n\\n## 3. Evaluation\\n\\nEvaluation is the final step in the machine learning process, where the model\'s performance is assessed to ensure it meets the desired criteria. This involves using various metrics to measure the accuracy, precision, recall, and other relevant aspects of the model.\\n\\n### Sub-steps:\\n\\n1. **Model Validation**: Using the validation set to tune the model and prevent overfitting.\\n2. **Performance Metrics**: Calculating metrics such as accuracy, precision, recall, F1 score, and AUC-ROC to evaluate the model\'s effectiveness.\\n3. **Cross-Validation**: Implementing techniques like k-fold cross-validation to ensure the model\'s robustness and reliability.\\n\\n### References:\\n\\n- [Scikit-Learn Documentation on Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\\n- [YouTube Video on Model Evaluation Metrics](https://www.youtube.com/watch?v=85dtiMz9tSo)\\n\\nUnderstanding the machine learning process is fundamental to developing effective models that can make accurate predictions and provide valuable insights."},{"id":"machine-learning/machine-learning-tools","metadata":{"permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-tools","source":"@site/blog/machine-learning/2024-05-26-machine-learning-tools.md","title":"Machine Learning Environment: Python, R, RStudio, and Colab","description":"Hi everyone! I\'m venturing into the exciting world of machine learning (ML), and this article details the tools I\'m using to get started.","date":"2024-05-26T00:00:00.000Z","tags":[{"label":"Machine Learning","permalink":"/gerardo-perrucci/blog/tags/machine-learning"},{"label":"python","permalink":"/gerardo-perrucci/blog/tags/python"},{"label":"R","permalink":"/gerardo-perrucci/blog/tags/r"},{"label":"RStudio","permalink":"/gerardo-perrucci/blog/tags/r-studio"},{"label":"Colab","permalink":"/gerardo-perrucci/blog/tags/colab"}],"readingTime":1.345,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"machine-learning/machine-learning-tools","title":"Machine Learning Environment: Python, R, RStudio, and Colab","authors":["me"],"tags":["Machine Learning","python","R","RStudio","Colab"]},"unlisted":false,"prevItem":{"title":"Machine Learning Process: A Comprehensive Guide","permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-process"},"nextItem":{"title":"New React Compiler in React 19","permalink":"/gerardo-perrucci/blog/react/new-compiler-react-19"}},"content":"Hi everyone! I\'m venturing into the exciting world of machine learning (ML), and this article details the tools I\'m using to get started.\\n\\n## Essential Software\\n\\n**Python:** As a widely used general-purpose language, Python is a popular choice for ML due to its readability, extensive libraries, and large community.\\n\\nPython download: https://www.python.org/downloads/\\n\\n**R:** Another powerful language specifically designed for statistics and data analysis. R offers a rich ecosystem of packages specifically tailored for ML tasks.\\n\\nYou can download R from the official website: https://www.r-project.org/\\n\\n**RStudio:** An integrated development environment (IDE) built specifically for R. It provides a user-friendly interface for writing, running, and managing your R code. It also offers features like code completion, syntax highlighting, and debugging tools, making your R experience smoother.\\n\\nDownload RStudio from the official website: https://www.rstudio.com/products/rstudio/\\n\\n\x3c!-- truncate --\x3e\\n\\n### Cloud Platform\\n\\n**Google Colab:** This fantastic platform offered by Google allows you to run Python or R code directly within your web browser. Colab provides free access to powerful hardware with GPUs (graphical processing units) that can significantly accelerate your ML computations, especially when dealing with large datasets. It\'s a great option if you don\'t have a powerful computer or prefer a cloud-based environment.\\n\\nAccess Google Colab at: Google Colab: https://colab.research.google.com/\\n\\n### Next Steps\\n\\nI\'ll delve into exploring some popular Python libraries for machine learning, such as NumPy, pandas, scikit-learn, and TensorFlow.\\n\\nBonus Tip: Jupyter Notebook is a web-based IDE that allows you to create and share documents that contain live code, equations, visualizations, and explanatory text. It\'s a great tool for documenting your ML projects and experiments.\\n\\nYou can download Jupyter Notebook: https://jupyter.org/"},{"id":"react/new-compiler-react-19","metadata":{"permalink":"/gerardo-perrucci/blog/react/new-compiler-react-19","source":"@site/blog/react/2024-05-26-new-compiler-react-19.mdx","title":"New React Compiler in React 19","description":"The new React compiler introduced in React 19 it will significantly improve React development.","date":"2024-05-26T00:00:00.000Z","tags":[{"label":"React","permalink":"/gerardo-perrucci/blog/tags/react"},{"label":"Compiler","permalink":"/gerardo-perrucci/blog/tags/compiler"},{"label":"React 19","permalink":"/gerardo-perrucci/blog/tags/react-19"}],"readingTime":1.785,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Perrucci","title":"Software Engineer","url":"https://github.com/centrodph","imageURL":"https://avatars.githubusercontent.com/u/2073951?v=4","key":"me"}],"frontMatter":{"slug":"react/new-compiler-react-19","title":"New React Compiler in React 19","authors":["me"],"tags":["React","Compiler","React 19"]},"unlisted":false,"prevItem":{"title":"Machine Learning Environment: Python, R, RStudio, and Colab","permalink":"/gerardo-perrucci/blog/machine-learning/machine-learning-tools"}},"content":"**The new React compiler introduced in React 19 it will significantly improve React development.**\\n\\nReact\'s new compiler is an innovative tool designed to automatically optimize your React applications. By deeply understanding your code, the compiler applies optimizations grounded in React\u2019s core principles. These optimizations can lead to significant performance enhancements, especially for complex applications.\\n\\nCurrently in its experimental phase, the new compiler has the potential to revolutionize React development. It\'s particularly interesting to see how it will interact with the `inline` optimization technique used in React like memo, useMemo useCallback.\\n\\nThe ongoing development and integration of the compiler promise exciting advancements in the efficiency and performance of React applications. As the tool matures, it could become a game-changer for developers seeking to build faster, more efficient applications.\\n\\n\x3c!-- truncate --\x3e\\n\\nSome bullet points to consider:\\n\\n    - A new experimental tool called the React compiler can automatically optimize your React application.\\n\\n    - It accomplishes this by thoroughly comprehending the code and applying optimizations based on React\'s principles.\\n\\n    - This can result in performance improvements, particularly for intricate applications.\\n\\n    - The compiler is still in its experimental phase, but it has the potential to revolutionize React development.\\n\\n    - It will be interesting to see how it affects the `inline` optimization technique used in React like memo, useMemo useCallback.\\n\\nSure, according to the document (https://react.dev/learn/react-compiler), the React compiler can optimize your React application in a few specific cases.\\n\\n**It can automatically memoize certain values or groups of values within your components and hooks.** This means it can cache the results of functions so that they don\'t have to be recalculated every time the component renders if the inputs haven\'t changed.\\n\\nIn addition, the compiler can skip over re-rendering components that haven\'t changed. For instance, if a parent component re-renders, it won\'t necessarily force all of its child components to re-render as well. Finally, is important to note that the code is expected to follow the **React Rules** in order to work properly with the compiler. Learn more about the React Rules: https://react.dev/reference/rules\\n\\n<iframe\\n  width=\\"560\\"\\n  height=\\"315\\"\\n  src=\\"https://www.youtube.com/embed/Xo-ddmNGjY8?si=EfLo6F1IM1O7PUll\\"\\n  title=\\"YouTube video player\\"\\n  frameborder=\\"0\\"\\n  allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\"\\n  referrerpolicy=\\"strict-origin-when-cross-origin\\"\\n  allowfullscreen\\n></iframe>"}]}}')}}]);