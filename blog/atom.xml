<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://centrodph.github.io/gerardo-perrucci/blog</id>
    <title>Gerardo Perrucci Blog</title>
    <updated>2024-05-27T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://centrodph.github.io/gerardo-perrucci/blog"/>
    <subtitle>Gerardo Perrucci Blog</subtitle>
    <icon>https://centrodph.github.io/gerardo-perrucci/img/2073951.jpeg</icon>
    <entry>
        <title type="html"><![CDATA[Machine Learning: Feature Scaling]]></title>
        <id>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process</id>
        <link href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process"/>
        <updated>2024-05-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Imagine you're a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual's absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.]]></summary>
        <content type="html"><![CDATA[<p>Imagine you're a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual's absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.</p>
<p><img decoding="async" loading="lazy" alt="Machine Learning Feature Scaling Source: someka.net" src="data:image/webp;base64,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" width="744" height="342" class="img_ev3q"></p>
<p><strong>Feature scaling</strong> is a data pre-processing technique that addresses this issue. It essentially standardizes the range of features in your dataset, ensuring all features contribute equally during model training. Let's delve deeper into why and how this works.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-scale">Why Scale?<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#why-scale" class="hash-link" aria-label="Direct link to Why Scale?" title="Direct link to Why Scale?">​</a></h3>
<ul>
<li><strong>Fair Play for All Features:</strong> Features with larger values can overshadow those with smaller ones, even if the smaller ones hold valuable information. Scaling creates a level playing field.</li>
<li><strong>Distance Matters:</strong> Many machine learning algorithms rely on calculating distances between data points. Feature scaling ensures these distances accurately reflect the underlying relationships.</li>
<li><strong>Faster &amp; More Efficient Learning:</strong> By putting features on a similar scale, the learning algorithm can converge (find an optimal solution) faster and more efficiently.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="normalization-vs-standardization-two-sides-of-the-scaling-coin">Normalization vs. Standardization: Two Sides of the Scaling Coin<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#normalization-vs-standardization-two-sides-of-the-scaling-coin" class="hash-link" aria-label="Direct link to Normalization vs. Standardization: Two Sides of the Scaling Coin" title="Direct link to Normalization vs. Standardization: Two Sides of the Scaling Coin">​</a></h3>
<p>Normalization and standardization are two common feature scaling techniques, and the terms are sometimes used interchangeably. However, there's a subtle difference:</p>
<ul>
<li>
<p><strong>Normalization:</strong> This technique scales features to a specific range, typically between 0 and 1 (Min-Max Scaling) or -1 and 1. It's useful when you know the data distribution or want to bound values within a specific range.</p>
<ul>
<li>Formula:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#d6deeb;--prism-background-color:#011627"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#d6deeb;background-color:#011627"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#d6deeb"><span class="token plain">X_scaled = (X - min(X)) / (max(X) - min(X))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Here,
_ X_scaled is the normalized feature
_ X is the original feature value
_ min(X) is the minimum value in the feature
_ max(X) is the maximum value in the feature</li>
</ul>
</li>
<li>
<p><strong>Standardization:</strong> This technique transforms features to have a mean of 0 and a standard deviation of 1 (Z-score normalization). It assumes a Gaussian (bell-shaped) distribution for the data and emphasizes outliers more than normalization.</p>
<ul>
<li>Formula:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#d6deeb;--prism-background-color:#011627"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#d6deeb;background-color:#011627"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#d6deeb"><span class="token plain">X_scaled = (X - mean(X)) / std(X)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Here,
_ X_scaled is the standardized feature
_ X is the original feature value
_ mean(X) is the average of all values in the feature
_ std(X) is the standard deviation of the feature</li>
</ul>
</li>
</ul>
<p><strong>Choosing the Right Technique:</strong></p>
<p>The best technique depends on your data and the specific algorithm you're using. Here's a general guideline:</p>
<ul>
<li>Use Min-Max scaling if the data distribution is unknown or outliers are not a concern.</li>
<li>Use standardization (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.</li>
</ul>
<p><strong>Examples:</strong></p>
<p>Imagine a dataset with two features: house price (in millions) and distance from a school (in meters). Without scaling, the massive price range would overpower the distance information. Scaling levels the field, allowing the model to learn from both features effectively.</p>
<p><strong>Further Learning:</strong></p>
<ul>
<li><a href="https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048" target="_blank" rel="noopener noreferrer">Feature Scaling and Why Does Machine Learning Need It</a></li>
<li><a href="https://www.geeksforgeeks.org/ml-feature-scaling-part-2/" target="_blank" rel="noopener noreferrer">Feature Engineering: Scaling, Normalization, and Standardization</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener noreferrer">Essence of Linear Algebra</a></li>
</ul>
<p>Remember, feature scaling is a crucial step in building robust and accurate machine learning models. By ensuring all features are on the same page, you can empower your models to learn from your data more effectively.</p>
<p><strong>Choosing the Right Technique:</strong></p>
<p>The best technique depends on your data and the specific algorithm you're using. Here's a general guideline:</p>
<ul>
<li>Use <code>normalization</code> scaling if the data distribution is unknown or outliers are not a concern.</li>
<li>Use <code>standardization</code> (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.</li>
</ul>
<p>I hope this addition clarifies the concepts of normalization and standardization with their respective formulas!</p>]]></content>
        <author>
            <name>Gerardo Perrucci</name>
            <uri>https://github.com/centrodph</uri>
        </author>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Data Preprocessing" term="Data Preprocessing"/>
        <category label="Feature Scaling" term="Feature Scaling"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Process: A Comprehensive Guide]]></title>
        <id>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process</id>
        <link href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process"/>
        <updated>2024-05-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) has become a cornerstone of modern technology, driving advancements in various fields such as healthcare, finance, and transportation. To build effective ML models, it's essential to understand the three main steps in the machine learning process: Data Preprocessing, Modeling, and Evaluation. This article breaks down these steps, detailing the sub-steps involved and providing references for further reading and understanding.]]></summary>
        <content type="html"><![CDATA[<p>Imagine you're a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual's absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.</p>
<p><img decoding="async" loading="lazy" alt="Machine Learning Feature Scaling Source: someka.net" src="data:image/webp;base64,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" width="744" height="342" class="img_ev3q"></p>
<p><strong>Feature scaling</strong> is a data pre-processing technique that addresses this issue. It essentially standardizes the range of features in your dataset, ensuring all features contribute equally during model training. Let's delve deeper into why and how this works.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-scale">Why Scale?<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#why-scale" class="hash-link" aria-label="Direct link to Why Scale?" title="Direct link to Why Scale?">​</a></h3>
<ul>
<li><strong>Fair Play for All Features:</strong> Features with larger values can overshadow those with smaller ones, even if the smaller ones hold valuable information. Scaling creates a level playing field.</li>
<li><strong>Distance Matters:</strong> Many machine learning algorithms rely on calculating distances between data points. Feature scaling ensures these distances accurately reflect the underlying relationships.</li>
<li><strong>Faster &amp; More Efficient Learning:</strong> By putting features on a similar scale, the learning algorithm can converge (find an optimal solution) faster and more efficiently.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="normalization-vs-standardization-two-sides-of-the-scaling-coin">Normalization vs. Standardization: Two Sides of the Scaling Coin<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#normalization-vs-standardization-two-sides-of-the-scaling-coin" class="hash-link" aria-label="Direct link to Normalization vs. Standardization: Two Sides of the Scaling Coin" title="Direct link to Normalization vs. Standardization: Two Sides of the Scaling Coin">​</a></h3>
<p>Normalization and standardization are two common feature scaling techniques, and the terms are sometimes used interchangeably. However, there's a subtle difference:</p>
<ul>
<li>
<p><strong>Normalization:</strong> This technique scales features to a specific range, typically between 0 and 1 (Min-Max Scaling) or -1 and 1. It's useful when you know the data distribution or want to bound values within a specific range.</p>
<ul>
<li>Formula:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#d6deeb;--prism-background-color:#011627"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#d6deeb;background-color:#011627"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#d6deeb"><span class="token plain">X_scaled = (X - min(X)) / (max(X) - min(X))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Here,
_ X_scaled is the normalized feature
_ X is the original feature value
_ min(X) is the minimum value in the feature
_ max(X) is the maximum value in the feature</li>
</ul>
</li>
<li>
<p><strong>Standardization:</strong> This technique transforms features to have a mean of 0 and a standard deviation of 1 (Z-score normalization). It assumes a Gaussian (bell-shaped) distribution for the data and emphasizes outliers more than normalization.</p>
<ul>
<li>Formula:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#d6deeb;--prism-background-color:#011627"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#d6deeb;background-color:#011627"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#d6deeb"><span class="token plain">X_scaled = (X - mean(X)) / std(X)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Here,
_ X_scaled is the standardized feature
_ X is the original feature value
_ mean(X) is the average of all values in the feature
_ std(X) is the standard deviation of the feature</li>
</ul>
</li>
</ul>
<p><strong>Choosing the Right Technique:</strong></p>
<p>The best technique depends on your data and the specific algorithm you're using. Here's a general guideline:</p>
<ul>
<li>Use Min-Max scaling if the data distribution is unknown or outliers are not a concern.</li>
<li>Use standardization (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.</li>
</ul>
<p><strong>Examples:</strong></p>
<p>Imagine a dataset with two features: house price (in millions) and distance from a school (in meters). Without scaling, the massive price range would overpower the distance information. Scaling levels the field, allowing the model to learn from both features effectively.</p>
<p><strong>Further Learning:</strong></p>
<ul>
<li><a href="https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048" target="_blank" rel="noopener noreferrer">Feature Scaling and Why Does Machine Learning Need It</a></li>
<li><a href="https://www.geeksforgeeks.org/ml-feature-scaling-part-2/" target="_blank" rel="noopener noreferrer">Feature Engineering: Scaling, Normalization, and Standardization</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener noreferrer">Essence of Linear Algebra</a></li>
</ul>
<p>Remember, feature scaling is a crucial step in building robust and accurate machine learning models. By ensuring all features are on the same page, you can empower your models to learn from your data more effectively.</p>
<p><strong>Choosing the Right Technique:</strong></p>
<p>The best technique depends on your data and the specific algorithm you're using. Here's a general guideline:</p>
<ul>
<li>Use <code>normalization</code> scaling if the data distribution is unknown or outliers are not a concern.</li>
<li>Use <code>standardization</code> (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.</li>
</ul>
<p>I hope this addition clarifies the concepts of normalization and standardization with their respective formulas!</p>]]></content>
        <author>
            <name>Gerardo Perrucci</name>
            <uri>https://github.com/centrodph</uri>
        </author>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Data Preprocessing" term="Data Preprocessing"/>
        <category label="Modeling" term="Modeling"/>
        <category label="Evaluation" term="Evaluation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Environment: Python, R, RStudio, and Colab]]></title>
        <id>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools</id>
        <link href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools"/>
        <updated>2024-05-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! I'm venturing into the exciting world of machine learning (ML), and this article details the tools I'm using to get started.]]></summary>
        <content type="html"><![CDATA[<p>Hi everyone! I'm venturing into the exciting world of machine learning (ML), and this article details the tools I'm using to get started.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="essential-software">Essential Software<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools#essential-software" class="hash-link" aria-label="Direct link to Essential Software" title="Direct link to Essential Software">​</a></h2>
<p><strong>Python:</strong> As a widely used general-purpose language, Python is a popular choice for ML due to its readability, extensive libraries, and large community.</p>
<p>Python download: <a href="https://www.python.org/downloads/" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a></p>
<p><strong>R:</strong> Another powerful language specifically designed for statistics and data analysis. R offers a rich ecosystem of packages specifically tailored for ML tasks.</p>
<p>You can download R from the official website: <a href="https://www.r-project.org/" target="_blank" rel="noopener noreferrer">https://www.r-project.org/</a></p>
<p><strong>RStudio:</strong> An integrated development environment (IDE) built specifically for R. It provides a user-friendly interface for writing, running, and managing your R code. It also offers features like code completion, syntax highlighting, and debugging tools, making your R experience smoother.</p>
<p>Download RStudio from the official website: <a href="https://www.rstudio.com/products/rstudio/" target="_blank" rel="noopener noreferrer">https://www.rstudio.com/products/rstudio/</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cloud-platform">Cloud Platform<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools#cloud-platform" class="hash-link" aria-label="Direct link to Cloud Platform" title="Direct link to Cloud Platform">​</a></h3>
<p><strong>Google Colab:</strong> This fantastic platform offered by Google allows you to run Python or R code directly within your web browser. Colab provides free access to powerful hardware with GPUs (graphical processing units) that can significantly accelerate your ML computations, especially when dealing with large datasets. It's a great option if you don't have a powerful computer or prefer a cloud-based environment.</p>
<p>Access Google Colab at: Google Colab: <a href="https://colab.research.google.com/" target="_blank" rel="noopener noreferrer">https://colab.research.google.com/</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="next-steps">Next Steps<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps">​</a></h3>
<p>I'll delve into exploring some popular Python libraries for machine learning, such as NumPy, pandas, scikit-learn, and TensorFlow.</p>
<p>Bonus Tip: Jupyter Notebook is a web-based IDE that allows you to create and share documents that contain live code, equations, visualizations, and explanatory text. It's a great tool for documenting your ML projects and experiments.</p>
<p>You can download Jupyter Notebook: <a href="https://jupyter.org/" target="_blank" rel="noopener noreferrer">https://jupyter.org/</a></p>]]></content>
        <author>
            <name>Gerardo Perrucci</name>
            <uri>https://github.com/centrodph</uri>
        </author>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="python" term="python"/>
        <category label="R" term="R"/>
        <category label="RStudio" term="RStudio"/>
        <category label="Colab" term="Colab"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[New React Compiler in React 19]]></title>
        <id>https://centrodph.github.io/gerardo-perrucci/blog/react/new-compiler-react-19</id>
        <link href="https://centrodph.github.io/gerardo-perrucci/blog/react/new-compiler-react-19"/>
        <updated>2024-05-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The new React compiler introduced in React 19 it will significantly improve React development.]]></summary>
        <content type="html"><![CDATA[<p><strong>The new React compiler introduced in React 19 it will significantly improve React development.</strong></p>
<p>React's new compiler is an innovative tool designed to automatically optimize your React applications. By deeply understanding your code, the compiler applies optimizations grounded in React’s core principles. These optimizations can lead to significant performance enhancements, especially for complex applications.</p>
<p>Currently in its experimental phase, the new compiler has the potential to revolutionize React development. It's particularly interesting to see how it will interact with the <code>inline</code> optimization technique used in React like memo, useMemo useCallback.</p>
<p>The ongoing development and integration of the compiler promise exciting advancements in the efficiency and performance of React applications. As the tool matures, it could become a game-changer for developers seeking to build faster, more efficient applications.</p>
<p>Some bullet points to consider:</p>
<ul>
<li>
<p>A new experimental tool called the React compiler can automatically optimize your React application.</p>
</li>
<li>
<p>It accomplishes this by thoroughly comprehending the code and applying optimizations based on React's principles.</p>
</li>
<li>
<p>This can result in performance improvements, particularly for intricate applications.</p>
</li>
<li>
<p>The compiler is still in its experimental phase, but it has the potential to revolutionize React development.</p>
</li>
<li>
<p>It will be interesting to see how it affects the <code>inline</code> optimization technique used in React like memo, useMemo useCallback.</p>
</li>
</ul>
<p>Sure, according to the document (<a href="https://react.dev/learn/react-compiler" target="_blank" rel="noopener noreferrer">https://react.dev/learn/react-compiler</a>), the React compiler can optimize your React application in a few specific cases.</p>
<p><strong>It can automatically memoize certain values or groups of values within your components and hooks.</strong> This means it can cache the results of functions so that they don't have to be recalculated every time the component renders if the inputs haven't changed.</p>
<p>In addition, the compiler can skip over re-rendering components that haven't changed. For instance, if a parent component re-renders, it won't necessarily force all of its child components to re-render as well. Finally, is important to note that the code is expected to follow the <strong>React Rules</strong> in order to work properly with the compiler. Learn more about the React Rules: <a href="https://react.dev/reference/rules" target="_blank" rel="noopener noreferrer">https://react.dev/reference/rules</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Xo-ddmNGjY8?si=EfLo6F1IM1O7PUll" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin"></iframe>]]></content>
        <author>
            <name>Gerardo Perrucci</name>
            <uri>https://github.com/centrodph</uri>
        </author>
        <category label="React" term="React"/>
        <category label="Compiler" term="Compiler"/>
        <category label="React 19" term="React 19"/>
    </entry>
</feed>