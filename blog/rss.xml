<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Gerardo Perrucci Blog</title>
        <link>https://centrodph.github.io/gerardo-perrucci/blog</link>
        <description>Gerardo Perrucci Blog</description>
        <lastBuildDate>Mon, 27 May 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Machine Learning: Feature Scaling]]></title>
            <link>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling</link>
            <guid>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling</guid>
            <pubDate>Mon, 27 May 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Imagine you're a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual's absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.]]></description>
            <content:encoded><![CDATA[<p>Imagine you're a teacher and your students are working on a group project. One student is a math whiz, another excels at writing, and a third is a history buff. If you grade each section based on the individual's absolute strengths, the math whiz would dominate the score, even if the writing and history were excellent. This is similar to what can happen in machine learning with features (data points) on vastly different scales.</p>
<p><img decoding="async" loading="lazy" alt="Machine Learning Feature Scaling Source: someka.net" src="data:image/webp;base64,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" width="744" height="342" class="img_ev3q"></p>
<p><strong>Feature scaling</strong> is a data pre-processing technique that addresses this issue. It essentially standardizes the range of features in your dataset, ensuring all features contribute equally during model training. Let's delve deeper into why and how this works.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-scale">Why Scale?<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling#why-scale" class="hash-link" aria-label="Direct link to Why Scale?" title="Direct link to Why Scale?">​</a></h3>
<ul>
<li><strong>Fair Play for All Features:</strong> Features with larger values can overshadow those with smaller ones, even if the smaller ones hold valuable information. Scaling creates a level playing field.</li>
<li><strong>Distance Matters:</strong> Many machine learning algorithms rely on calculating distances between data points. Feature scaling ensures these distances accurately reflect the underlying relationships.</li>
<li><strong>Faster &amp; More Efficient Learning:</strong> By putting features on a similar scale, the learning algorithm can converge (find an optimal solution) faster and more efficiently.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="normalization-vs-standardization-two-sides-of-the-scaling-coin">Normalization vs. Standardization: Two Sides of the Scaling Coin<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-feature-scaling#normalization-vs-standardization-two-sides-of-the-scaling-coin" class="hash-link" aria-label="Direct link to Normalization vs. Standardization: Two Sides of the Scaling Coin" title="Direct link to Normalization vs. Standardization: Two Sides of the Scaling Coin">​</a></h3>
<p>Normalization and standardization are two common feature scaling techniques, and the terms are sometimes used interchangeably. However, there's a subtle difference:</p>
<ul>
<li>
<p><strong>Normalization:</strong> This technique scales features to a specific range, typically between 0 and 1 (Min-Max Scaling) or -1 and 1. It's useful when you know the data distribution or want to bound values within a specific range.</p>
<ul>
<li>Formula:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#d6deeb;--prism-background-color:#011627"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#d6deeb;background-color:#011627"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#d6deeb"><span class="token plain">X_scaled = (X - min(X)) / (max(X) - min(X))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Here,
_ X_scaled is the normalized feature
_ X is the original feature value
_ min(X) is the minimum value in the feature
_ max(X) is the maximum value in the feature</li>
</ul>
</li>
<li>
<p><strong>Standardization:</strong> This technique transforms features to have a mean of 0 and a standard deviation of 1 (Z-score normalization). It assumes a Gaussian (bell-shaped) distribution for the data and emphasizes outliers more than normalization.</p>
<ul>
<li>Formula:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#d6deeb;--prism-background-color:#011627"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#d6deeb;background-color:#011627"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#d6deeb"><span class="token plain">X_scaled = (X - mean(X)) / std(X)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Here,
_ X_scaled is the standardized feature
_ X is the original feature value
_ mean(X) is the average of all values in the feature
_ std(X) is the standard deviation of the feature</li>
</ul>
</li>
</ul>
<p><strong>Choosing the Right Technique:</strong></p>
<p>The best technique depends on your data and the specific algorithm you're using. Here's a general guideline:</p>
<ul>
<li>Use Min-Max scaling if the data distribution is unknown or outliers are not a concern.</li>
<li>Use standardization (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.</li>
</ul>
<p><strong>Examples:</strong></p>
<p>Imagine a dataset with two features: house price (in millions) and distance from a school (in meters). Without scaling, the massive price range would overpower the distance information. Scaling levels the field, allowing the model to learn from both features effectively.</p>
<p><strong>Further Learning:</strong></p>
<ul>
<li><a href="https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048" target="_blank" rel="noopener noreferrer">Feature Scaling and Why Does Machine Learning Need It</a></li>
<li><a href="https://www.geeksforgeeks.org/ml-feature-scaling-part-2/" target="_blank" rel="noopener noreferrer">Feature Engineering: Scaling, Normalization, and Standardization</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener noreferrer">Essence of Linear Algebra</a></li>
</ul>
<p>Remember, feature scaling is a crucial step in building robust and accurate machine learning models. By ensuring all features are on the same page, you can empower your models to learn from your data more effectively.</p>
<p><strong>Choosing the Right Technique:</strong></p>
<p>The best technique depends on your data and the specific algorithm you're using. Here's a general guideline:</p>
<ul>
<li>Use <code>normalization</code> scaling if the data distribution is unknown or outliers are not a concern.</li>
<li>Use <code>standardization</code> (Z-score) if the data is assumed to be Gaussian distributed or you want to emphasize the impact of outliers.</li>
</ul>
<p>I hope this addition clarifies the concepts of normalization and standardization with their respective formulas!</p>]]></content:encoded>
            <category>Machine Learning</category>
            <category>Data Preprocessing</category>
            <category>Feature Scaling</category>
        </item>
        <item>
            <title><![CDATA[Machine Learning Process: A Comprehensive Guide]]></title>
            <link>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process</link>
            <guid>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process</guid>
            <pubDate>Mon, 27 May 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Machine learning (ML) has become a cornerstone of modern technology, driving advancements in various fields such as healthcare, finance, and transportation. To build effective ML models, it's essential to understand the three main steps in the machine learning process: Data Preprocessing, Modeling, and Evaluation. This article breaks down these steps, detailing the sub-steps involved and providing references for further reading and understanding.]]></description>
            <content:encoded><![CDATA[<p>Machine learning (ML) has become a cornerstone of modern technology, driving advancements in various fields such as healthcare, finance, and transportation. To build effective ML models, it's essential to understand the three main steps in the machine learning process: Data Preprocessing, Modeling, and Evaluation. This article breaks down these steps, detailing the sub-steps involved and providing references for further reading and understanding.</p>
<p><img decoding="async" loading="lazy" alt="Machine Learning Process" src="https://centrodph.github.io/gerardo-perrucci/assets/images/ml-process-52fb963fdb615006e0c3bd3cfb5b3f50.png" width="1600" height="800" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-data-preprocessing">1. Data Preprocessing<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#1-data-preprocessing" class="hash-link" aria-label="Direct link to 1. Data Preprocessing" title="Direct link to 1. Data Preprocessing">​</a></h2>
<p>Data preprocessing is the first and arguably the most crucial step in the machine learning pipeline. This step ensures that the data is clean, consistent, and suitable for the modeling process.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sub-steps">Sub-steps:<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#sub-steps" class="hash-link" aria-label="Direct link to Sub-steps:" title="Direct link to Sub-steps:">​</a></h3>
<ol>
<li><strong>Data Collection</strong>: Gathering relevant data from various sources. This can include databases, APIs, and web scraping.</li>
<li><strong>Data Cleaning</strong>: Removing or correcting any inaccuracies in the data, such as missing values, outliers, and duplicates.</li>
<li><strong>Data Transformation</strong>: Converting data into a suitable format for analysis, which might involve normalization, standardization, or encoding categorical variables.</li>
<li><strong>Data Splitting</strong>: Dividing the data into training, validation, and test sets to evaluate the model's performance.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="references">References:<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#references" class="hash-link" aria-label="Direct link to References:" title="Direct link to References:">​</a></h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank" rel="noopener noreferrer">Scikit-Learn Documentation on Data Preprocessing</a></li>
<li><a href="https://www.kaggle.com/learn/data-cleaning" target="_blank" rel="noopener noreferrer">Kaggle Data Preprocessing Tutorial</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-modeling">2. Modeling<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#2-modeling" class="hash-link" aria-label="Direct link to 2. Modeling" title="Direct link to 2. Modeling">​</a></h2>
<p>Once the data is preprocessed, the next step is to build and train the machine learning model. This involves selecting the appropriate algorithm and fine-tuning it to achieve the best performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sub-steps-1">Sub-steps:<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#sub-steps-1" class="hash-link" aria-label="Direct link to Sub-steps:" title="Direct link to Sub-steps:">​</a></h3>
<ol>
<li><strong>Algorithm Selection</strong>: Choosing a machine learning algorithm based on the problem type (e.g., regression, classification, clustering).</li>
<li><strong>Model Training</strong>: Feeding the training data into the algorithm to learn the underlying patterns and relationships.</li>
<li><strong>Hyperparameter Tuning</strong>: Adjusting the algorithm's parameters to optimize performance. This can be done using techniques like grid search or random search.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="references-1">References:<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#references-1" class="hash-link" aria-label="Direct link to References:" title="Direct link to References:">​</a></h3>
<ul>
<li><a href="https://scikit-learn.org/stable/supervised_learning.html" target="_blank" rel="noopener noreferrer">Scikit-Learn Documentation on Supervised Learning</a></li>
<li><a href="https://www.tensorflow.org/guide/keras/train_and_evaluate" target="_blank" rel="noopener noreferrer">TensorFlow Model Training Guide</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-evaluation">3. Evaluation<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#3-evaluation" class="hash-link" aria-label="Direct link to 3. Evaluation" title="Direct link to 3. Evaluation">​</a></h2>
<p>Evaluation is the final step in the machine learning process, where the model's performance is assessed to ensure it meets the desired criteria. This involves using various metrics to measure the accuracy, precision, recall, and other relevant aspects of the model.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sub-steps-2">Sub-steps:<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#sub-steps-2" class="hash-link" aria-label="Direct link to Sub-steps:" title="Direct link to Sub-steps:">​</a></h3>
<ol>
<li><strong>Model Validation</strong>: Using the validation set to tune the model and prevent overfitting.</li>
<li><strong>Performance Metrics</strong>: Calculating metrics such as accuracy, precision, recall, F1 score, and AUC-ROC to evaluate the model's effectiveness.</li>
<li><strong>Cross-Validation</strong>: Implementing techniques like k-fold cross-validation to ensure the model's robustness and reliability.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="references-2">References:<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-process#references-2" class="hash-link" aria-label="Direct link to References:" title="Direct link to References:">​</a></h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank" rel="noopener noreferrer">Scikit-Learn Documentation on Model Evaluation</a></li>
<li><a href="https://www.youtube.com/watch?v=85dtiMz9tSo" target="_blank" rel="noopener noreferrer">YouTube Video on Model Evaluation Metrics</a></li>
</ul>
<p>Understanding the machine learning process is fundamental to developing effective models that can make accurate predictions and provide valuable insights.</p>]]></content:encoded>
            <category>Machine Learning</category>
            <category>Data Preprocessing</category>
            <category>Modeling</category>
            <category>Evaluation</category>
        </item>
        <item>
            <title><![CDATA[Machine Learning Environment: Python, R, RStudio, and Colab]]></title>
            <link>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools</link>
            <guid>https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools</guid>
            <pubDate>Sun, 26 May 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Hi everyone! I'm venturing into the exciting world of machine learning (ML), and this article details the tools I'm using to get started.]]></description>
            <content:encoded><![CDATA[<p>Hi everyone! I'm venturing into the exciting world of machine learning (ML), and this article details the tools I'm using to get started.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="essential-software">Essential Software<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools#essential-software" class="hash-link" aria-label="Direct link to Essential Software" title="Direct link to Essential Software">​</a></h2>
<p><strong>Python:</strong> As a widely used general-purpose language, Python is a popular choice for ML due to its readability, extensive libraries, and large community.</p>
<p>Python download: <a href="https://www.python.org/downloads/" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a></p>
<p><strong>R:</strong> Another powerful language specifically designed for statistics and data analysis. R offers a rich ecosystem of packages specifically tailored for ML tasks.</p>
<p>You can download R from the official website: <a href="https://www.r-project.org/" target="_blank" rel="noopener noreferrer">https://www.r-project.org/</a></p>
<p><strong>RStudio:</strong> An integrated development environment (IDE) built specifically for R. It provides a user-friendly interface for writing, running, and managing your R code. It also offers features like code completion, syntax highlighting, and debugging tools, making your R experience smoother.</p>
<p>Download RStudio from the official website: <a href="https://www.rstudio.com/products/rstudio/" target="_blank" rel="noopener noreferrer">https://www.rstudio.com/products/rstudio/</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cloud-platform">Cloud Platform<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools#cloud-platform" class="hash-link" aria-label="Direct link to Cloud Platform" title="Direct link to Cloud Platform">​</a></h3>
<p><strong>Google Colab:</strong> This fantastic platform offered by Google allows you to run Python or R code directly within your web browser. Colab provides free access to powerful hardware with GPUs (graphical processing units) that can significantly accelerate your ML computations, especially when dealing with large datasets. It's a great option if you don't have a powerful computer or prefer a cloud-based environment.</p>
<p>Access Google Colab at: Google Colab: <a href="https://colab.research.google.com/" target="_blank" rel="noopener noreferrer">https://colab.research.google.com/</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="next-steps">Next Steps<a href="https://centrodph.github.io/gerardo-perrucci/blog/machine-learning/machine-learning-tools#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps">​</a></h3>
<p>I'll delve into exploring some popular Python libraries for machine learning, such as NumPy, pandas, scikit-learn, and TensorFlow.</p>
<p>Bonus Tip: Jupyter Notebook is a web-based IDE that allows you to create and share documents that contain live code, equations, visualizations, and explanatory text. It's a great tool for documenting your ML projects and experiments.</p>
<p>You can download Jupyter Notebook: <a href="https://jupyter.org/" target="_blank" rel="noopener noreferrer">https://jupyter.org/</a></p>]]></content:encoded>
            <category>Machine Learning</category>
            <category>python</category>
            <category>R</category>
            <category>RStudio</category>
            <category>Colab</category>
        </item>
        <item>
            <title><![CDATA[New React Compiler in React 19]]></title>
            <link>https://centrodph.github.io/gerardo-perrucci/blog/react/new-compiler-react-19</link>
            <guid>https://centrodph.github.io/gerardo-perrucci/blog/react/new-compiler-react-19</guid>
            <pubDate>Sun, 26 May 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[The new React compiler introduced in React 19 it will significantly improve React development.]]></description>
            <content:encoded><![CDATA[<p><strong>The new React compiler introduced in React 19 it will significantly improve React development.</strong></p>
<p>React's new compiler is an innovative tool designed to automatically optimize your React applications. By deeply understanding your code, the compiler applies optimizations grounded in React’s core principles. These optimizations can lead to significant performance enhancements, especially for complex applications.</p>
<p>Currently in its experimental phase, the new compiler has the potential to revolutionize React development. It's particularly interesting to see how it will interact with the <code>inline</code> optimization technique used in React like memo, useMemo useCallback.</p>
<p>The ongoing development and integration of the compiler promise exciting advancements in the efficiency and performance of React applications. As the tool matures, it could become a game-changer for developers seeking to build faster, more efficient applications.</p>
<p>Some bullet points to consider:</p>
<ul>
<li>
<p>A new experimental tool called the React compiler can automatically optimize your React application.</p>
</li>
<li>
<p>It accomplishes this by thoroughly comprehending the code and applying optimizations based on React's principles.</p>
</li>
<li>
<p>This can result in performance improvements, particularly for intricate applications.</p>
</li>
<li>
<p>The compiler is still in its experimental phase, but it has the potential to revolutionize React development.</p>
</li>
<li>
<p>It will be interesting to see how it affects the <code>inline</code> optimization technique used in React like memo, useMemo useCallback.</p>
</li>
</ul>
<p>Sure, according to the document (<a href="https://react.dev/learn/react-compiler" target="_blank" rel="noopener noreferrer">https://react.dev/learn/react-compiler</a>), the React compiler can optimize your React application in a few specific cases.</p>
<p><strong>It can automatically memoize certain values or groups of values within your components and hooks.</strong> This means it can cache the results of functions so that they don't have to be recalculated every time the component renders if the inputs haven't changed.</p>
<p>In addition, the compiler can skip over re-rendering components that haven't changed. For instance, if a parent component re-renders, it won't necessarily force all of its child components to re-render as well. Finally, is important to note that the code is expected to follow the <strong>React Rules</strong> in order to work properly with the compiler. Learn more about the React Rules: <a href="https://react.dev/reference/rules" target="_blank" rel="noopener noreferrer">https://react.dev/reference/rules</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Xo-ddmNGjY8?si=EfLo6F1IM1O7PUll" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin"></iframe>]]></content:encoded>
            <category>React</category>
            <category>Compiler</category>
            <category>React 19</category>
        </item>
    </channel>
</rss>